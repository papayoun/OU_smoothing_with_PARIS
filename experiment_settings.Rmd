---
title: "Smoothing in partially observed OU process"
author: "Pierre Gloaguen"
date: "25/11/2020"
output: 
  pdf_document:
    keep_tex: true
    dev: cairo_pdf
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = FALSE, message = FALSE,
                      cache = TRUE,
                      fig.align = "center")
library(tidyverse)
```

```{r ggplot_theme, cache = FALSE}
my_theme <- function(base_size = 8){
  theme_bw()  %+replace%
    theme(
      panel.border = element_rect(colour = "black", 
                                  fill = rgb(0, 0, 0, 0)),
      # plot.background = element_rect(fill = "white"),# bg around panel
      legend.background = element_blank(), 
      text = element_text(family = "LM Roman 10", size = base_size),
      axis.title = element_text(size = rel(1)),
      legend.text = element_text(size = rel(1)),
      legend.title = element_text(size = rel(1)),
      axis.text = element_text(size = rel(1)),
      strip.background = element_rect(fill = "lightgoldenrod1",
                                      color = "black"),
      plot.subtitle = element_text(hjust = 0.5, size = rel(1)),
      plot.title = element_text(face = "bold", size = rel(1.2), hjust = 0.5))
}
theme_set(my_theme(base_size = 12))
```

# True model

\begin{align}
\text{d} X(t) &= -\rho(X(t) - \mu) \text{d}t + \sigma \text{d}W(t),~X(0) = x(0)~t\geq 0\\
Y(k) &= X(k) + \sigma_{obs}\varepsilon_k,~ \varepsilon_k \overset{i.i.d}{\sim} \mathcal{N}(0, 1)
\end{align}

- Data are generated according to this true model, giving a vector $y_{0:T}$ from which inference must be performed.

# Biased model

We consider a bias version of the true model. This model is biased in two ways:

1. In the dynamics model, which is approximated through an Euler scheme (and setting that  $t_{k + 1} = t_k + \delta$). 
2. In the observation model, where it is supposed that only a fraction $\alpha_{obs}$ of $X(t_k)$ is observed.
\begin{align}
X(t_k + \delta) &= -\rho(X(t_k) - \mu)\times \delta + \sigma\sqrt{\delta} \times \tilde{\varepsilon}_k, X(0) = x(0), ~k =  0, 1,..., \tilde{\varepsilon}_k \overset{i.i.d}{\sim} \mathcal{N}(0, 1)\\
Y(t_k) &= \alpha_{obs}X(t_k) + \sigma_{obs}\varepsilon_k,~k = 0, 1, ..., \varepsilon_k \overset{i.i.d}{\sim} \mathcal{N}(0, 1)
\end{align}

We here emphasize that this scenario is actually quite common, for instance in ecology, where the continuous time dynamics model is approximated in discrete time, and  supposed observation model is subject to some bias (we refer the interested reader to the literature of state space models for population dynamics).

```{r settings, echo = FALSE}
library(tidyverse)
load("experiments_results.RData")
```


# Smoothing

- In this context, our goal is to estimate, for each $n\leq T$ the realization on $y_{0:n}$ of the random variable:
$$\Phi_n = \mathbb{E}\left[\sum_{k = 0}^{n} X(t_k)\vert Y_{0:n}\right].$$

- It is known that in this context, the actual marginal distribution is Gaussian with parameters that can be obtained through Kalman recursions. The quantity $\Phi_n$ can then be computed explicitly.
- We consider three alternatives estimation of $\Phi_n$ which are:
    - *PaRIS:* The online estimation is obtained with the PaRIS method. The model used in the recursions is the true one, and the proposal is the optimal filter;
    - *Biased PaRIS:* The online estimation is obtained with the PaRIS method. However, here, the model used in the recursions is the biased one, and the proposal is the optimal filter relative to this biased model; 
    - *Biased Kalman:* The estimation obtained with the Kalman recursions when the true model is replaced by the biased model;

- For each estimator $\hat{\Phi}_n$, we compute

- The scenario is performed with $T = `r n_obs - 1`,~\delta = `r delta`,~\rho = `r rho`,~\mu = `r mu`,~x(0) = `r m1`,~\sigma = `r sigma`,~\sigma_{obs} = `r round(sigma_obs, 1)`,~\alpha_{obs} = `r my_H`$, using 200 particles for the particle filter, and obtaining two backward samples at each backward sampling step.

# Results

```{r plot_results}
results %>% 
  ggplot() + 
  aes(x = obs_index, y = -difference, color = Smoother) +
  geom_line(aes(group = interaction(Smoother, Replicate)), alpha = .5) +
  geom_line(data = filter(results, Smoother == "Biased Kalman"), size = 1.5) +
  geom_hline(yintercept = 0, linetype = 2, color = "red") + 
  labs(x = "n", y = expression(hat(Phi)[n]~"-"~Phi[n]))
```

We can see here that using the biased model (either with Kalman recursions or PaRIS), the bias is our estimate only grows linearly with $n$, as expected by results EQREF. In this context, the bias due to Monte Carlo error is not visible.

<!-- # The $\mathbf{L}_n$ kernel in hierarchical linear Gaussian model -->

<!-- Suppose we have observations $y_1,\dots, y_n$ recorded at ordered times $\mathbf{T} = (t_1,\dots, t_n)$ which are supposed to be realizations of the following hierarchical linear Gaussian model: -->

<!-- \begin{align} -->
<!-- X(t + \delta) &= a_\delta X(t) + b_\delta \varepsilon_t \\ -->
<!-- Y(t + \delta) &= c X(t + \delta) + d \epsilon_t, -->
<!-- \end{align} -->

<!-- where $\left\lbrace \varepsilon_t, \epsilon_t \right\rbrace_{t\in \mathbf{T}}$ is a sequence of mutually independant standard Gaussian random variables. -->

<!-- Following the papers notations, we have that: -->

<!-- \begin{align*} -->
<!-- q(x_t, x_{t + \delta}) &=\frac{1}{\sqrt{2\pi}b_\delta}\exp\left\lbrace-\frac{1}{2b^2_\delta} \left( x_{t+\delta} - a_\delta x_t \right)^2 \right\rbrace\\ -->
<!-- g(x_{t + \delta}, y_{t + \delta}) &= \frac{1}{\sqrt{2\pi}d} \exp\left\lbrace -\frac{1}{2d^2} \left( y_{t+\delta} - c x_{t + \delta} \right)^2\right\rbrace -->
<!-- \end{align*} -->

<!-- For this model, we define the unnormalized kernel $\mathbf{L}_n$ -->
<!-- $$\mathsf{X} \times \mathcal{X} \owns (x_{t_n}, A) \mapsto \int_A \ell(x_{t_n}, x_{t_n + \delta}, y_{t_n + \delta}) \text{d} x_{t + \delta}$$ -->
<!-- where $\ell(x_t, x_{t + \delta}, y_{t + \delta}) = q(x_t, x_{t + \delta})g(x_{t + \delta}, y_{t + \delta})$. -->
<!-- We here note that $\mathbf{L}_n$ is implicitly dependant on $y_{t_n + \delta}$ and on all the model parameters $\theta = \left(a_\delta, b_\delta, c, d \right)$. -->

<!-- By standard conjugation of Gaussian random variables, one can check that we have: -->
<!-- $$\mathbf{L}_n\mathbf{1}_\mathsf{X}(x_{t_n}) = \frac{\gamma_\delta}{\sqrt{2 \pi}b_\delta d}\exp\left\lbrace -\frac{1}{2}\left(\frac{y^2_{t_n + \delta}}{d^2} + \frac{a_\delta^2x_{t_n}^2}{b_\delta^2}  - \gamma_\delta^2\times \left(\frac{c y_{t_n + \delta}}{d^2} + \frac{a_\delta x_{t_n}}{b_\delta^2} \right)^2 \right)   \right\rbrace, $$ -->
<!-- where  -->
<!-- $$\gamma^2_\delta = \left(\frac{1}{b_\delta^2 + \frac{c^2}{d^2}} \right)^{-1}$$ -->

<!-- And we then have, for any function $h \in \mathsf{F}(\mathcal{X})$: -->
<!-- $$\mathbf{L}_n h (x_{t_n}) = \mathbf{L}_n\mathbf{1}_\mathsf{X}(x_{t_n}) \times \mathbb{E}\left[h(X)\right],$$ -->
<!-- where  -->
<!-- $$X \sim \mathcal{N}\left(\gamma^2_\delta \left(\frac{c y_{t_n + \delta}}{d^2} + \frac{a_\delta x_{t_n}}{b_\delta^2} \right); \gamma_\delta^2\right).$$ -->

<!-- # Distance between original and skewed model -->

<!-- Suppose that: -->
<!-- $$ -->
<!-- \text{d} X(t) = -\rho X(t) \text{d}t + \text{d}W(t),~X(0) = x(0),~t\geq 0. -->
<!-- $$ -->
<!-- Moreover, we suppose this process is observed at times $\mathbf{T} = (0, \delta, \dots, n\delta)$ with a centered error of variance 1. This leads to the following model, that we'll denote $M_\star$ -->

<!-- \begin{align} -->
<!-- X(t + \delta) &=  \overbrace{\text{e}^{-\rho\delta}}^{a_\delta} X(t) + \overbrace{\sqrt{\frac{1}{2\rho}\left(1 - \text{e}^{-2\rho\delta}\right)}}^{b_\delta} \varepsilon_t \\ -->
<!-- Y(t + \delta) &= \underbrace{1}_c \times X(t + \delta) + \underbrace{1}_d \times \epsilon_t, -->
<!-- \end{align} -->

<!-- Suppose that we would use an Euler scheme (with time step $\delta$) to approximate the transition of the orginal model, suppose, moreover, that the $c$ parameter is misspecified and set to $1 -\nu$^for some $0< \nu < 1$. We therefore have the following skewed model, denoted $M_{\delta, \nu}$: -->
<!-- \begin{align} -->
<!-- X(t + \delta) &=  \overbrace{\left(1 - \rho\delta\right)}^{a_\delta} X(t) + \overbrace{\sqrt{\delta}}^{b_\delta} \varepsilon_t \\ -->
<!-- Y(t + \delta) &= \underbrace{\left(1 - \nu\right)}_c \times X(t + \delta) + \underbrace{1}_d \times \epsilon_t, -->
<!-- \end{align} -->

# Bias in the observation model

Consider the following model:

\begin{align}
X(t + \delta) &= \text{e}^{-\frac{1}{2} \delta} X(t) + \sqrt{1 - \text{e}^{-\delta}} E_t \\
Y(t + \delta) &= ( 1 - \varepsilon) \times X(t + \delta) + E^{'}_t,
\end{align}

where $\left\lbrace E_t, E^{'}_t \right\rbrace_{t\in \mathbf{T}}$ is a sequence of mutually independant standard Gaussian random variables, and $\varepsilon \in [0,1]$ is a bias parameter:

Following the paper notations, we have that:

\begin{align*}
q(x_t, x_{t + \delta}) &=\frac{1}{\sqrt{2\pi\times (1 - \text{e}^{-\delta})}}\exp\left\lbrace-\frac{1}{2(1 - \text{e}^{-\delta})} \left( x_{t+\delta} - \text{e}^{-\frac{1}{2} \delta} x_t \right)^2 \right\rbrace\\
g(x_{t + \delta}, y_{t + \delta},\varepsilon) &= \frac{1}{\sqrt{2\pi}} \exp\left\lbrace -\frac{1}{2} \left( y_{t+\delta} - (1 - \varepsilon) x_{t + \delta} \right)^2\right\rbrace;
\end{align*}

One can see that $g(x_{t + \delta}, y_{t + \delta}, \varepsilon)$ is Lipschitz  in its $\varepsilon$ (as its derivative is bounded) argument, and morever that :
$$\left\vert g(x_{t + \delta}, y_{t + \delta},\varepsilon) - g(x_{t + \delta}, y_{t + \delta},0)\right\vert \leq \sup_{0\leq \varepsilon \leq 1} \left\vert\frac{\delta}{\delta \varepsilon} g(x_{t + \delta}, y_{t + \delta},\varepsilon)\right\vert \times \varepsilon
\leq \overbrace{\left( \left\vert x_{t + \delta}y_{t + \delta} - x^2_{t + \delta} \right\vert + x^2_{t + \delta}\right)}^{:= k(x_{t + \delta}, y_{t + \delta})} \varepsilon.$$
Therefore, we have that:

\begin{align*}
\left\vert \mathbf{L}_t^\varepsilon h(x_t) - \mathbf{L}_t h(x_t)  \right\vert &= \left\vert \int q(x_t, x_{t + \delta} )\left( g(x_{t + \delta}, y_{t + \delta},\varepsilon) - g(x_{t + \delta}, y_{t + \delta}, 0)\right) h(x_{t+\delta})\text{d}x_{t + \delta}  \right\vert\\
&\leq \varepsilon \int q(x_t, x_{t + \delta} ) k(x_{t + \delta},y_{t + \delta}) \vert h(x_{t+\delta})\vert  \text{d} x_{t + \delta}.
\end{align*}
Therefore, the integral must be uniformly bounded for every $x_\delta$. 
In the case where 
$h(x) = x \mathbf{1}_{\vert x \vert \leq M}$, then $k(x, y)\vert h(x) \vert \leq M(2M^2 + M\vert y\vert)$, and we have 
$$\left\vert \mathbf{L}_t^\varepsilon h(x_t) - \mathbf{L}_t h(x_t)  \right\vert \leq M(2M^2 + M\vert y_{t + \delta}\vert)\varepsilon$$